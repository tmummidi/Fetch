{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276633a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tm37384/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tm37384/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tm37384/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to the 'Description' column\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed_Description\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Use TF-IDF vectorizer on the processed descriptions\u001b[39;00m\n\u001b[1;32m     34\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import difflib\n",
    "import nltk\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set up stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Initialize Dash app with Bootstrap theme\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "\n",
    "# Function to calculate similarity score\n",
    "def similarity_score(offer1, offer2):\n",
    "    seq = difflib.SequenceMatcher(None, offer1, offer2)\n",
    "    return seq.ratio()\n",
    "\n",
    "# Function to remove duplicates with a higher score\n",
    "def remove_duplicates_with_higher_score(recommendations):\n",
    "    unique_offers = {}\n",
    "\n",
    "    for offer, score in recommendations:\n",
    "        duplicate_found = False\n",
    "        for existing_offer, existing_score in unique_offers.items():\n",
    "            if similarity_score(offer, existing_offer) > 0.8:\n",
    "                if score > existing_score:\n",
    "                    unique_offers[offer] = score\n",
    "                    del unique_offers[existing_offer]\n",
    "                duplicate_found = True\n",
    "                break\n",
    "\n",
    "        if not duplicate_found:\n",
    "            unique_offers[offer] = score\n",
    "\n",
    "    unique_recommendations = list(unique_offers.items())\n",
    "\n",
    "    return unique_recommendations\n",
    "\n",
    "# Function to load data from CSV files\n",
    "def load_data(offer_path, brand_category_path, categories_path):\n",
    "    df = pd.read_csv(offer_path)\n",
    "    df1 = pd.read_csv(brand_category_path)\n",
    "    df2 = pd.read_csv(categories_path)\n",
    "\n",
    "    df1 = pd.merge(df1, df2[['PRODUCT_CATEGORY', 'IS_CHILD_CATEGORY_TO']], how='left',\n",
    "                   left_on='BRAND_BELONGS_TO_CATEGORY', right_on='PRODUCT_CATEGORY')\n",
    "    df = pd.merge(df, df1[['BRAND', 'BRAND_BELONGS_TO_CATEGORY', 'IS_CHILD_CATEGORY_TO']], on='BRAND', how='left')\n",
    "\n",
    "    return df, df1, df2\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(str(text))\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to extract keywords\n",
    "def extract_keywords(input_text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([input_text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to recommend offers\n",
    "def recommend_offers(input_text, tfidf_vectorizer, tfidf_matrix, df):\n",
    "    input_text = preprocess_text(input_text)\n",
    "    input_vector = tfidf_vectorizer.transform([input_text])\n",
    "\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix, input_vector).flatten()\n",
    "    related_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "    input_keywords = extract_keywords(input_text)\n",
    "\n",
    "    unique_offers = set()\n",
    "    recommendations = []\n",
    "\n",
    "    for idx in related_indices:\n",
    "        offer_keywords = extract_keywords(df['combined_text'].iloc[idx])\n",
    "        brand_retailer_match = any(keyword in offer_keywords for keyword in input_keywords)\n",
    "\n",
    "        if brand_retailer_match:\n",
    "            offer_name = df['OFFER'].iloc[idx]\n",
    "            similarity_score = cosine_similarities[idx]\n",
    "\n",
    "            if offer_name not in unique_offers:\n",
    "                recommendations.append((offer_name, similarity_score))\n",
    "                unique_offers.add(offer_name)\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Function to update output\n",
    "def update_output(query):\n",
    "    recommendations = recommend_offers(query, tfidf_vectorizer, tfidf_matrix, df)\n",
    "    unique_recommendations = remove_duplicates_with_higher_score(recommendations)\n",
    "    \n",
    "    if unique_recommendations:\n",
    "        return [html.P(f\"Offer: {offer}, Similarity Score: {score}\") for offer, score in unique_recommendations]\n",
    "    else:\n",
    "        return [html.P(\"No relevant offers found.\")]\n",
    "\n",
    "# Define app layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"OFFER RECOMMENDATION DASHBOARD\", style={'textAlign': 'center'}),\n",
    "\n",
    "    dbc.Input(id='user-input', type='text', placeholder='Enter your search query', style={'width': '80%', 'margin': '10px'}),\n",
    "    \n",
    "    dbc.Button('Submit', id='submit-button', n_clicks=0, style={'margin': '10px'}),\n",
    "\n",
    "    dcc.Loading(\n",
    "        id=\"loading\",\n",
    "        type=\"circle\",\n",
    "        children=[html.Div(id='output-div', style={'margin': '20px', 'padding': '10px', 'border': '1px solid #ddd'})]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Callback to update output div\n",
    "@app.callback(\n",
    "    Output('output-div', 'children'),\n",
    "    [Input('submit-button', 'n_clicks')],\n",
    "    [dash.dependencies.State('user-input', 'value')]\n",
    ")\n",
    "def update_output_div(n_clicks, value):\n",
    "    if n_clicks > 0:\n",
    "        return update_output(value)\n",
    "\n",
    "# Load data and run the app\n",
    "if __name__ == '__main__':\n",
    "    offer_path = 'offer_retailer.csv'\n",
    "    brand_category_path = 'brand_category.csv'\n",
    "    categories_path = 'categories.csv'\n",
    "\n",
    "    try:\n",
    "        df, df1, df2 = load_data(offer_path, brand_category_path, categories_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: One or more files not found. Please check the file paths.\")\n",
    "        raise\n",
    "\n",
    "    df['processed_offer'] = df['OFFER'].apply(preprocess_text)\n",
    "    df['processed_retailer'] = df['RETAILER'].apply(preprocess_text)\n",
    "    df['processed_brand'] = df['BRAND'].apply(preprocess_text)\n",
    "    df['processed_BRAND_BELONGS_TO_CATEGORY'] = df['BRAND_BELONGS_TO_CATEGORY'].apply(preprocess_text)\n",
    "    df['processed_IS_CHILD_CATEGORY_TO'] = df['IS_CHILD_CATEGORY_TO'].apply(preprocess_text)\n",
    "    df['combined_text'] = df[['processed_offer', 'processed_retailer', 'processed_brand',\n",
    "                              'processed_BRAND_BELONGS_TO_CATEGORY', 'processed_IS_CHILD_CATEGORY_TO']].agg(' '.join, axis=1)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'].astype(str))\n",
    "\n",
    "    host = '127.0.0.1'\n",
    "    port = 9030  # Change port if there is a connection issue\n",
    "\n",
    "    print(f\" * Running on http://{host}:{port}/ (Press CTRL+C to quit)\")\n",
    "    \n",
    "    app.run_server(debug=True, port=9030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
